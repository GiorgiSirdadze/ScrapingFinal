# News Scraping & Analysis Project

## ðŸ“° Overview

This project is a Python-based multi-source news scraping and analysis system. It collects article data from three major websites using different scraping techniques and provides insightful analysis through charts, summaries, and automated reports.

---

## âœ… Features

- Scrapes data from:
  - BBC (RSS Feed - static)
  - CNN (JavaScript - dynamic via Selenium)
  - TechCrunch (Structured crawler via Scrapy)
- Three scraping techniques:
  - BeautifulSoup4
  - Selenium
  - Scrapy
- Stores articles in a SQLite database
- Advanced data analysis:
  - Keyword trends per source
  - Named Entity Recognition (NER)
  - Sentiment analysis of titles
  - Cross-source similarity detection
- Data export:
  - CSV, JSON, Excel
- Visualizations:
  - Bar chart, Pie chart
  - Auto-generated HTML report
- Interactive CLI interface (menu-based)

---

## ðŸ“¦ Installation

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python -m textblob.download_corpora
```

(Optional)

```bash
pip install .
```

---

## âš™ï¸ Project Structure

```
final-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/       # scraping logic and factory pattern
â”‚   â”œâ”€â”€ data/           # DB interface
â”‚   â”œâ”€â”€ analysis/       # stats, reports, charts
â”‚   â”œâ”€â”€ cli/            # CLI logic (optional)
â”‚   â””â”€â”€ utils/          # config loader, logger, helpers
â”œâ”€â”€ config/             # YAML settings and sources
â”œâ”€â”€ data_output/        # all exports and reports
â”œâ”€â”€ scrapy_crawler/     # Scrapy project for TechCrunch
â”œâ”€â”€ docs/               # architecture, guide, API ref
â”œâ”€â”€ tests/              # unit tests
â”œâ”€â”€ main.py             # entry point (interactive CLI)
â”œâ”€â”€ setup.py            # package installer (optional)
â””â”€â”€ requirements.txt
```

---

## ðŸš€ Usage

### 1. Run Scrapy crawler first:

```bash
cd src/scrapers
scrapy crawl techcrunch -o techcrunch.json
cd ../../../
```

### 2. Launch the CLI

```bash
python main.py
```

Then choose an option:

```
1. Scrape all sources
2. Run advanced analysis
3. Export data
4. Generate charts and HTML report
5. Exit
```

---

## ðŸ“Š Output

- `news.db`: stored articles (SQLite)
- `data_output/reports/articles.csv/json/xlsx`: exports
- `data_output/reports/plots/`: chart images
- `data_output/reports/report.html`: auto-generated report

---

## ðŸ§ª Testing

Run unit tests:

```bash
pytest tests/
```

Includes:

- DB creation test
- Table existence check

---

## ðŸ“„ Documentation

See `docs/` folder:

- `architecture.md`
- `user_guide.md`
- `api_reference.md`

---

## ðŸ‘¨â€ðŸ’» Author

Made with â¤ï¸ by **Giorgi**  
Part of the final project for Python Data Scraping course (2025)

---

## âœ… Status

**COMPLETE** â€” all project requirements from the PDF have been satisfied and exceeded.

---

## ðŸ Final Notes

- Factory pattern is used for scraper selection
- Configuration via YAML
- Designed with modularity and clarity
- Easy to demo live or record

> "Three websites. Three scraping methods. One clean pipeline."
