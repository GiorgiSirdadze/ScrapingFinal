from src.data.database import init_db, save_articles
from src.scrapers.static_scraper import scrape_bbc_rss
from src.scrapers.selenium_scraper import scrape_cnn_headlines
from src.analysis.statistics import run_advanced_analysis
from src.analysis.exporter import export_articles
from src.analysis.reports import generate_charts, generate_html_report
import json
import os
import time

def load_techcrunch_articles():
    path = "src/scrapers/tc_crawler/techcrunch.json"
    if not os.path.exists(path):
        print("âŒ TechCrunch JSON not found. Run `scrapy crawl techcrunch` first.")
        return []
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def scrape_all():
    init_db()

    bbc = scrape_bbc_rss()
    print(f"ğŸ“° BBC: {len(bbc)} articles scraped.")
    save_articles(bbc, "BBC")

    cnn = scrape_cnn_headlines()
    print(f"ğŸ“° CNN: {len(cnn)} articles scraped.")
    save_articles(cnn, "CNN")

    tc = load_techcrunch_articles()
    print(f"ğŸ“° TechCrunch: {len(tc)} articles loaded from JSON.")
    save_articles(tc, "TechCrunch")

def show_menu():
    while True:
        print("\nğŸ“‹ What would you like to do?")
        print("1. Scrape all sources")
        print("2. Run advanced analysis")
        print("3. Export data (CSV, JSON, Excel)")
        print("4. Generate charts and HTML report")
        print("5. Exit")

        choice = input("Enter your choice (1â€“5): ").strip()

        if choice == "1":
            scrape_all()
        elif choice == "2":
            run_advanced_analysis()
        elif choice == "3":
            export_articles()
        elif choice == "4":
            generate_charts()
            generate_html_report()
        elif choice == "5":
            print("ğŸ‘‹ Exiting... Goodbye!")
            time.sleep(1)
            break
        else:
            print("âŒ Invalid choice. Try again.")

if __name__ == "__main__":
    show_menu()
